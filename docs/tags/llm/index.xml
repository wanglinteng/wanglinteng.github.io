<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on LINTENG&#39;S BLOG</title>
    <link>/tags/llm/</link>
    <description>Recent content in LLM on LINTENG&#39;S BLOG</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Aug 2023 00:00:00 +0000</lastBuildDate><atom:link href="/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>openai chatgpt token 是如何计算的？</title>
      <link>/post/llm_token/</link>
      <pubDate>Wed, 30 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/llm_token/</guid>
      <description>token是什么？
  大模型训练、推理本质是数值计算，文字需要转换为数字，通过文字转换得到的数字即为token。
  openai计费单位，按照输入token，输出token数量计算费用。 计费价格，注意 input token 和 output token价格不同。
    token是怎么计算的？
通过openai提供的在线工具测试英文、中文 token结果如下：
  英文
TOKEN IDS [31373, 644, 338, 534, 1438, 30] 一个有用的经验法则是，对于普通英语文本，一个token通常对应约4个字符的文本。这大约相当于一个单词的3/4（因此100个token~=75个单词）。
  中文
中文分词一个字可能对应多个token，所以可视化后可能出现乱码。
  tiktoken
tiktoken是openai开源的token工具，
1）生成 算法工程师 token id list。
2）对每一个token还原可以得到token id代表的字节，例如：163 代表 b&#39;\xe7&#39;。
对比算法工程师 utf-8 字节，可以发现与上述token id转换得到的字节一致。
import tiktoken text = &amp;#34;算法工程师&amp;#34; enc = tiktoken.encoding_for_model(&amp;#34;text-davinci-003&amp;#34;) token_list = enc.encode(text) print(&amp;#34;token_list&amp;#34;, token_list) for x in token_list: print(x, enc.</description>
    </item>
    
    <item>
      <title>LLM Embedding</title>
      <link>/post/llm_embedding/</link>
      <pubDate>Wed, 07 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/llm_embedding/</guid>
      <description>LLM Embedding ChatGLM   ChatGLM获取Embedding向量
inputs = tokenizer([text], return_tensors=&amp;#34;pt&amp;#34;).to(device) resp = model.transformer(**inputs, output_hidden_states=True) y = resp.last_hidden_state y_mean = torch.mean(y, dim=0, keepdim=True) y_mean = y_mean.cpu().detach().numpy() print(y_mean.shape) &amp;gt; [1, 4096]   </description>
    </item>
    
    <item>
      <title>LLM Develop Tools</title>
      <link>/post/llm_develop_tools/</link>
      <pubDate>Wed, 31 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/llm_develop_tools/</guid>
      <description>&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;如何开发以下需求？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;业务 （Models、Prompt）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;多模型（gpt-3.5, gpt-4, chatglm-6b, chatglm-6b-ft）&lt;/li&gt;
&lt;li&gt;多业务（运营push、个性化push、weibo地域、商业评论、新闻评论……）&lt;/li&gt;
&lt;li&gt;不同业务 X 不同模型 = 不同prompt&lt;/li&gt;
&lt;li&gt;敏感词风控&lt;/li&gt;
&lt;li&gt;输出标准化json格式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;网页 （Memory）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;多轮对话&lt;/li&gt;
&lt;li&gt;控制记忆上限（按轮数、按token[全量, 摘要……]）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;外部数据 （Indexes）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;汽车博文生成，需要参考车型各类参数（pdf，md, txt……）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;任务拆分 （Chain）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;汽车博文3段式生成
&lt;ol&gt;
&lt;li&gt;根据brief =》开头&lt;/li&gt;
&lt;li&gt;根据brief、开头 =》 中间&lt;/li&gt;
&lt;li&gt;根据开头、中间 =》结尾&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;日志（Callbacks）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;过程日志统计、token统计……&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;……&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;工具&#34;&gt;工具&lt;/h2&gt;
&lt;p&gt;目前开源工具包括以下几种：llama_index、langchain、semantic-kernel等。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
